# gRPC ì„œë²„ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ì„±ëŠ¥ ìµœì í™” ê°œìš”](#ì„±ëŠ¥-ìµœì í™”-ê°œìš”)
2. [HTTP/2 ìµœì í™”](#http2-ìµœì í™”)
3. [Protocol Buffers ìµœì í™”](#protocol-buffers-ìµœì í™”)
4. [ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”](#ë°ì´í„°ë² ì´ìŠ¤-ìµœì í™”)
5. [ìºì‹± ì „ëµ](#ìºì‹±-ì „ëµ)
6. [ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§](#ëª¨ë‹ˆí„°ë§-ë°-í”„ë¡œíŒŒì¼ë§)

## ğŸ¯ ì„±ëŠ¥ ìµœì í™” ê°œìš”

### í˜„ì¬ ì„±ëŠ¥ ëª©í‘œ
```
gRPC Server Performance Targets:
â”œâ”€â”€ RPS: 10,000+ requests/sec
â”œâ”€â”€ ì§€ì—°ì‹œê°„: <50ms p95, <100ms p99
â”œâ”€â”€ ë™ì‹œ ì—°ê²°: 5,000+ connections
â”œâ”€â”€ ë©”ëª¨ë¦¬: <2GB for 1000 concurrent users
â”œâ”€â”€ CPU: <70% utilization under full load
â””â”€â”€ ì²˜ë¦¬ëŸ‰: 100MB/s sustained
```

### ìµœì í™” ì „ëµ ê³„ì¸µ
```
Optimization Layers:
â”œâ”€â”€ Protocol Level (HTTP/2, Protocol Buffers)
â”œâ”€â”€ Application Level (Connection pooling, Async I/O)
â”œâ”€â”€ Data Level (Database, Cache, Serialization)
â”œâ”€â”€ Infrastructure Level (Load balancing, CDN)
â””â”€â”€ Monitoring Level (Observability, Alerting)
```

## âš¡ HTTP/2 ìµœì í™”

### 1. ì—°ê²° ê´€ë¦¬ ìµœì í™”

```rust
// src/server/connection_pool.rs
use tonic::transport::Server;
use tower::ServiceBuilder;
use tower_http::trace::TraceLayer;
use std::time::Duration;

pub struct OptimizedGrpcServer {
    connection_config: ConnectionConfig,
    pool_manager: Arc<ConnectionPoolManager>,
    metrics_collector: Arc<MetricsCollector>,
}

#[derive(Debug, Clone)]
pub struct ConnectionConfig {
    pub max_concurrent_streams: u32,
    pub initial_window_size: u32,
    pub max_frame_size: u32,
    pub keepalive_interval: Duration,
    pub keepalive_timeout: Duration,
    pub tcp_keepalive: Duration,
    pub tcp_nodelay: bool,
    pub http2_adaptive_window: bool,
}

impl Default for ConnectionConfig {
    fn default() -> Self {
        Self {
            max_concurrent_streams: 1000,        // HTTP/2 ìŠ¤íŠ¸ë¦¼ ìˆ˜ ì œí•œ
            initial_window_size: 1 << 20,        // 1MB ì´ˆê¸° ìœˆë„ìš°
            max_frame_size: 1 << 14,             // 16KB ìµœëŒ€ í”„ë ˆì„
            keepalive_interval: Duration::from_secs(30),
            keepalive_timeout: Duration::from_secs(5),
            tcp_keepalive: Duration::from_secs(60),
            tcp_nodelay: true,                   // Nagle ì•Œê³ ë¦¬ì¦˜ ë¹„í™œì„±í™”
            http2_adaptive_window: true,         // ì ì‘í˜• ìœˆë„ìš° í¬ê¸°
        }
    }
}

impl OptimizedGrpcServer {
    pub fn new(config: ConnectionConfig) -> Self {
        Self {
            connection_config: config,
            pool_manager: Arc::new(ConnectionPoolManager::new()),
            metrics_collector: Arc::new(MetricsCollector::new()),
        }
    }
    
    pub async fn build_server(&self) -> Result<Server, Box<dyn std::error::Error + Send + Sync>> {
        let mut server = Server::builder()
            // HTTP/2 ì„¤ì • ìµœì í™”
            .http2_keepalive_interval(Some(self.connection_config.keepalive_interval))
            .http2_keepalive_timeout(Some(self.connection_config.keepalive_timeout))
            .http2_adaptive_window(self.connection_config.http2_adaptive_window)
            .initial_stream_window_size(Some(self.connection_config.initial_window_size))
            .initial_connection_window_size(Some(self.connection_config.initial_window_size))
            .max_frame_size(Some(self.connection_config.max_frame_size))
            
            // TCP ì„¤ì • ìµœì í™”
            .tcp_keepalive(Some(self.connection_config.tcp_keepalive))
            .tcp_nodelay(self.connection_config.tcp_nodelay)
            
            // ë™ì‹œì„± ì œí•œ
            .concurrency_limit_per_connection(self.connection_config.max_concurrent_streams)
            
            // íƒ€ì„ì•„ì›ƒ ì„¤ì •
            .timeout(Duration::from_secs(30))
            
            // ì••ì¶• í™œì„±í™”
            .accept_compressed(tonic::codec::CompressionEncoding::Gzip);
        
        // ë¯¸ë“¤ì›¨ì–´ ìŠ¤íƒ êµ¬ì„±
        let service_stack = ServiceBuilder::new()
            .layer(TraceLayer::new_for_grpc())
            .layer(tower::limit::ConcurrencyLimitLayer::new(10000)) // ì „ì—­ ë™ì‹œì„± ì œí•œ
            .layer(tower::timeout::TimeoutLayer::new(Duration::from_secs(30)))
            .layer(tower::load_shed::LoadShedLayer::new())
            .layer(self.build_metrics_layer())
            .layer(self.build_auth_layer());
        
        Ok(server)
    }
    
    /// ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë ˆì´ì–´
    fn build_metrics_layer(&self) -> MetricsLayer {
        let collector = self.metrics_collector.clone();
        
        MetricsLayer::new(move |req| {
            let start_time = std::time::Instant::now();
            let collector_clone = collector.clone();
            
            async move {
                let result = req.await;
                let duration = start_time.elapsed();
                
                collector_clone.record_request_duration(duration);
                match &result {
                    Ok(_) => collector_clone.increment_success_counter(),
                    Err(_) => collector_clone.increment_error_counter(),
                }
                
                result
            }
        })
    }
    
    /// ì¸ì¦ ë ˆì´ì–´
    fn build_auth_layer(&self) -> AuthLayer {
        AuthLayer::new(move |req| {
            // JWT í† í° ê²€ì¦ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
            async move {
                // ìºì‹œëœ í† í° ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
                if let Some(auth_header) = req.headers().get("authorization") {
                    if let Ok(token) = auth_header.to_str() {
                        // Redisì— ìºì‹œëœ í† í° ê²€ì¦ ê²°ê³¼ ì‚¬ìš©
                        return validate_cached_token(token).await;
                    }
                }
                
                // ê³µê°œ ì—”ë“œí¬ì¸íŠ¸ëŠ” í†µê³¼
                if is_public_endpoint(req.uri().path()) {
                    return Ok(req);
                }
                
                Err(tonic::Status::unauthenticated("Token required"))
            }
        })
    }
}
```

### 2. ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”

```rust
// src/streaming/optimized_stream.rs
use tonic::{Request, Response, Status, Streaming};
use tokio_stream::{wrappers::ReceiverStream, StreamExt};
use std::pin::Pin;

pub struct OptimizedStreamHandler {
    buffer_size: usize,
    batch_size: usize,
    flush_interval: Duration,
    compression_enabled: bool,
}

impl OptimizedStreamHandler {
    pub fn new() -> Self {
        Self {
            buffer_size: 8192,                    // 8KB ë²„í¼
            batch_size: 100,                      // 100ê°œ ë©”ì‹œì§€ ë°°ì¹˜
            flush_interval: Duration::from_millis(10), // 10ms í”ŒëŸ¬ì‹œ ê°„ê²©
            compression_enabled: true,
        }
    }
    
    /// í´ë¼ì´ì–¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
    pub async fn handle_client_stream<T>(
        &self,
        mut stream: Streaming<T>,
    ) -> Result<Vec<T>, Status>
    where
        T: prost::Message + Default + Clone,
    {
        let mut messages = Vec::with_capacity(self.batch_size);
        let mut buffer = Vec::with_capacity(self.buffer_size);
        
        // ìŠ¤íŠ¸ë¦¼ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        while let Some(message) = stream.next().await {
            match message {
                Ok(msg) => {
                    messages.push(msg);
                    
                    // ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬
                    if messages.len() >= self.batch_size {
                        self.process_message_batch(&mut messages, &mut buffer).await?;
                        messages.clear();
                    }
                }
                Err(e) => {
                    tracing::error!("Stream error: {:?}", e);
                    return Err(e);
                }
            }
        }
        
        // ë‚¨ì€ ë©”ì‹œì§€ ì²˜ë¦¬
        if !messages.is_empty() {
            self.process_message_batch(&mut messages, &mut buffer).await?;
        }
        
        Ok(buffer)
    }
    
    /// ì„œë²„ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
    pub async fn create_server_stream<T>(
        &self,
        data: Vec<T>,
    ) -> Pin<Box<dyn Stream<Item = Result<T, Status>> + Send>>
    where
        T: Send + Clone + 'static,
    {
        let (tx, rx) = tokio::sync::mpsc::channel(self.buffer_size);
        let batch_size = self.batch_size;
        let flush_interval = self.flush_interval;
        
        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ì „ì†¡
        tokio::spawn(async move {
            let mut batch = Vec::with_capacity(batch_size);
            let mut flush_timer = tokio::time::interval(flush_interval);
            
            for item in data {
                batch.push(item);
                
                // ë°°ì¹˜ê°€ ê°€ë“ ì°¨ê±°ë‚˜ íƒ€ì´ë¨¸ê°€ ë§Œë£Œë˜ë©´ ì „ì†¡
                if batch.len() >= batch_size {
                    Self::send_batch(&tx, &mut batch).await;
                }
                
                // ì£¼ê¸°ì ìœ¼ë¡œ í”ŒëŸ¬ì‹œ
                tokio::select! {
                    _ = flush_timer.tick() => {
                        if !batch.is_empty() {
                            Self::send_batch(&tx, &mut batch).await;
                        }
                    }
                    _ = tokio::task::yield_now() => {}
                }
            }
            
            // ë‚¨ì€ ë°°ì¹˜ ì „ì†¡
            if !batch.is_empty() {
                Self::send_batch(&tx, &mut batch).await;
            }
        });
        
        Box::pin(ReceiverStream::new(rx))
    }
    
    /// ë°°ì¹˜ ì „ì†¡
    async fn send_batch<T>(
        sender: &tokio::sync::mpsc::Sender<Result<T, Status>>,
        batch: &mut Vec<T>,
    ) where
        T: Clone,
    {
        for item in batch.drain(..) {
            if sender.send(Ok(item)).await.is_err() {
                tracing::warn!("Failed to send stream item: receiver dropped");
                break;
            }
        }
    }
    
    /// ë©”ì‹œì§€ ë°°ì¹˜ ì²˜ë¦¬
    async fn process_message_batch<T>(
        &self,
        messages: &mut Vec<T>,
        buffer: &mut Vec<T>,
    ) -> Result<(), Status>
    where
        T: Clone,
    {
        // ë³‘ë ¬ ì²˜ë¦¬
        let tasks: Vec<_> = messages.chunks(10).map(|chunk| {
            let chunk = chunk.to_vec();
            tokio::spawn(async move {
                // ê° ì²­í¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
                Self::process_chunk(chunk).await
            })
        }).collect();
        
        // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        for task in tasks {
            match task.await {
                Ok(Ok(processed)) => buffer.extend(processed),
                Ok(Err(e)) => return Err(e),
                Err(e) => return Err(Status::internal(format!("Task error: {:?}", e))),
            }
        }
        
        Ok(())
    }
    
    /// ì²­í¬ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    async fn process_chunk<T>(chunk: Vec<T>) -> Result<Vec<T>, Status>
    where
        T: Clone,
    {
        // ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
        tokio::time::sleep(Duration::from_micros(100)).await; // ì‹œë®¬ë ˆì´ì…˜
        Ok(chunk)
    }
}
```

## ğŸ”§ Protocol Buffers ìµœì í™”

### 1. ë©”ì‹œì§€ ì„¤ê³„ ìµœì í™”

```protobuf
// proto/optimized_messages.proto
syntax = "proto3";

package optimized;

// ìµœì í™”ëœ ë©”ì‹œì§€ ì„¤ê³„ ì›ì¹™:
// 1. ìì£¼ ì‚¬ìš©ë˜ëŠ” í•„ë“œëŠ” ë‚®ì€ ë²ˆí˜¸ (1-15: 1ë°”ì´íŠ¸ ì¸ì½”ë”©)
// 2. ì˜µì…˜ í•„ë“œëŠ” ë†’ì€ ë²ˆí˜¸
// 3. ì¤‘ì²© ë©”ì‹œì§€ë³´ë‹¤ëŠ” í‰ë©´ êµ¬ì¡° ì„ í˜¸
// 4. ë°˜ë³µ í•„ë“œëŠ” packed=true ì‚¬ìš©

message OptimizedUserInfo {
  // ìì£¼ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ í•„ë“œë“¤ (1-15)
  int32 user_id = 1;           // 1ë°”ì´íŠ¸ ì¸ì½”ë”©
  string username = 2;         // 1ë°”ì´íŠ¸ ì¸ì½”ë”©
  string nickname = 3;         // 1ë°”ì´íŠ¸ ì¸ì½”ë”©
  int32 level = 4;             // 1ë°”ì´íŠ¸ ì¸ì½”ë”©
  int64 last_login = 5;        // 1ë°”ì´íŠ¸ ì¸ì½”ë”©
  
  // ëœ ì¤‘ìš”í•œ í•„ë“œë“¤ (16+)
  string email = 16;           // 2ë°”ì´íŠ¸ ì¸ì½”ë”©
  string profile_image = 17;
  repeated string tags = 18;   // packed ì ìš© ë¶ˆê°€ (string)
  
  // ìˆ«ì ë°°ì—´ì€ packed ìµœì í™”
  repeated int32 achievements = 19 [packed=true];  // ê³µê°„ ì ˆì•½
  repeated int32 friend_ids = 20 [packed=true];
  
  // í° ë°ì´í„°ëŠ” ë³„ë„ ìš”ì²­ìœ¼ë¡œ ë¶„ë¦¬ ê³ ë ¤
  // bytes profile_data = 21;  // í° ë°ì´í„°ëŠ” ë³„ë„ APIë¡œ
}

message OptimizedGameState {
  // í•„ìˆ˜ í•„ë“œë“¤
  int32 room_id = 1;
  int32 game_phase = 2;
  int32 round_number = 3;
  int32 remaining_time = 4;
  
  // í”Œë ˆì´ì–´ ì •ë³´ (ê²½ëŸ‰í™”)
  repeated LightPlayerInfo players = 5;
  
  // ì ìˆ˜ ì •ë³´
  GameScore score = 6;
  
  // ìµœê·¼ ì´ë²¤íŠ¸ë§Œ (ì „ì²´ ì´ë²¤íŠ¸ëŠ” ë³„ë„ ìŠ¤íŠ¸ë¦¼)
  repeated GameEvent recent_events = 7; // ìµœëŒ€ 10ê°œë¡œ ì œí•œ
}

message LightPlayerInfo {
  int32 user_id = 1;
  string nickname = 2;
  int32 role = 3;
  int32 status = 4;
  
  // ìœ„ì¹˜ ì •ë³´ (float â†’ int32ë¡œ ì •ë°€ë„ ì¡°ì •)
  int32 pos_x = 5;  // ì‹¤ì œ ì¢Œí‘œ * 1000 (ë°€ë¦¬ë¯¸í„° ì •ë°€ë„)
  int32 pos_y = 6;
  int32 pos_z = 7;
  int32 rotation = 8;
  
  // í†µê³„ (í•µì‹¬ë§Œ)
  int32 kills = 9;
  int32 deaths = 10;
  int32 score = 11;
}

// ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ìœ„í•œ ì²­í¬ ë©”ì‹œì§€
message DataChunk {
  string chunk_id = 1;
  int32 sequence = 2;
  int32 total_chunks = 3;
  bytes data = 4;
  string checksum = 5; // ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
}

// ì••ì¶•ëœ ë°°ì¹˜ ìš”ì²­
message BatchRequest {
  string request_id = 1;
  repeated google.protobuf.Any requests = 2; // ì—¬ëŸ¬ ìš”ì²­ì„ ë°°ì¹˜ë¡œ
  bool enable_compression = 3;
  int32 priority = 4; // 0=ë†’ìŒ, 1=ë³´í†µ, 2=ë‚®ìŒ
}

message BatchResponse {
  string request_id = 1;
  repeated google.protobuf.Any responses = 2;
  repeated Error errors = 3; // ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬
  int64 processing_time_ms = 4;
}
```

### 2. ì§ë ¬í™”/ì—­ì§ë ¬í™” ìµœì í™”

```rust
// src/serialization/optimized_proto.rs
use prost::{Message, DecodeError, EncodeError};
use std::sync::Arc;

pub struct OptimizedProtoHandler {
    buffer_pool: Arc<BufferPool>,
    compression_enabled: bool,
    cache: Arc<ProtoCache>,
}

struct BufferPool {
    small_buffers: crossbeam_queue::SegQueue<Vec<u8>>,  // < 1KB
    medium_buffers: crossbeam_queue::SegQueue<Vec<u8>>, // 1KB - 10KB
    large_buffers: crossbeam_queue::SegQueue<Vec<u8>>,  // > 10KB
}

impl BufferPool {
    fn new() -> Self {
        let pool = Self {
            small_buffers: crossbeam_queue::SegQueue::new(),
            medium_buffers: crossbeam_queue::SegQueue::new(),
            large_buffers: crossbeam_queue::SegQueue::new(),
        };
        
        // ë²„í¼ ë¯¸ë¦¬ í• ë‹¹
        for _ in 0..100 {
            pool.small_buffers.push(Vec::with_capacity(1024));
            pool.medium_buffers.push(Vec::with_capacity(10 * 1024));
            pool.large_buffers.push(Vec::with_capacity(100 * 1024));
        }
        
        pool
    }
    
    fn get_buffer(&self, size_hint: usize) -> Vec<u8> {
        let buffer = if size_hint < 1024 {
            self.small_buffers.pop()
        } else if size_hint < 10 * 1024 {
            self.medium_buffers.pop()
        } else {
            self.large_buffers.pop()
        };
        
        buffer.unwrap_or_else(|| Vec::with_capacity(size_hint.max(1024)))
    }
    
    fn return_buffer(&self, mut buffer: Vec<u8>) {
        buffer.clear();
        let capacity = buffer.capacity();
        
        if capacity < 1024 {
            let _ = self.small_buffers.push(buffer);
        } else if capacity < 10 * 1024 {
            let _ = self.medium_buffers.push(buffer);
        } else if capacity < 100 * 1024 {
            let _ = self.large_buffers.push(buffer);
        }
        // ë„ˆë¬´ í° ë²„í¼ëŠ” ë²„ë¦¼
    }
}

impl OptimizedProtoHandler {
    pub fn new() -> Self {
        Self {
            buffer_pool: Arc::new(BufferPool::new()),
            compression_enabled: true,
            cache: Arc::new(ProtoCache::new()),
        }
    }
    
    /// ìµœì í™”ëœ ì§ë ¬í™”
    pub fn encode_optimized<T: Message>(&self, message: &T) -> Result<Vec<u8>, EncodeError> {
        let size_hint = message.encoded_len();
        let mut buffer = self.buffer_pool.get_buffer(size_hint);
        
        // ì§ì ‘ ë²„í¼ì— ì¸ì½”ë”©
        message.encode(&mut buffer)?;
        
        // ì••ì¶• ì ìš© (í° ë©”ì‹œì§€ë§Œ)
        if self.compression_enabled && buffer.len() > 1024 {
            let compressed = self.compress_data(&buffer)?;
            self.buffer_pool.return_buffer(buffer);
            Ok(compressed)
        } else {
            Ok(buffer)
        }
    }
    
    /// ìµœì í™”ëœ ì—­ì§ë ¬í™”
    pub fn decode_optimized<T: Message + Default>(
        &self, 
        data: &[u8]
    ) -> Result<T, DecodeError> {
        // ì••ì¶• í•´ì œ í™•ì¸
        let decompressed_data = if self.is_compressed(data) {
            self.decompress_data(data)?
        } else {
            data.to_vec()
        };
        
        // ìºì‹œì—ì„œ í™•ì¸ (ìì£¼ ì‚¬ìš©ë˜ëŠ” ë©”ì‹œì§€)
        if let Some(cached) = self.cache.get::<T>(&decompressed_data) {
            return Ok(cached);
        }
        
        // ì—­ì§ë ¬í™”
        let message = T::decode(&*decompressed_data)?;
        
        // ìºì‹œì— ì €ì¥ (ì‘ì€ ë©”ì‹œì§€ë§Œ)
        if decompressed_data.len() < 1024 {
            self.cache.put(&decompressed_data, message.clone());
        }
        
        Ok(message)
    }
    
    /// ë°°ì¹˜ ì§ë ¬í™”
    pub fn encode_batch<T: Message>(
        &self, 
        messages: &[T]
    ) -> Result<Vec<u8>, EncodeError> {
        let total_size: usize = messages.iter().map(|m| m.encoded_len()).sum();
        let mut buffer = self.buffer_pool.get_buffer(total_size + messages.len() * 4); // í—¤ë” ê³µê°„
        
        // ë©”ì‹œì§€ ê°œìˆ˜ ì¸ì½”ë”©
        (messages.len() as u32).encode(&mut buffer)?;
        
        // ê° ë©”ì‹œì§€ ê¸¸ì´ + ë°ì´í„° ì¸ì½”ë”©
        for message in messages {
            let message_size = message.encoded_len() as u32;
            message_size.encode(&mut buffer)?;
            message.encode(&mut buffer)?;
        }
        
        Ok(buffer)
    }
    
    /// ë°°ì¹˜ ì—­ì§ë ¬í™”
    pub fn decode_batch<T: Message + Default>(
        &self, 
        data: &[u8]
    ) -> Result<Vec<T>, DecodeError> {
        let mut cursor = std::io::Cursor::new(data);
        
        // ë©”ì‹œì§€ ê°œìˆ˜ ì½ê¸°
        let count = u32::decode(&mut cursor)? as usize;
        let mut messages = Vec::with_capacity(count);
        
        // ê° ë©”ì‹œì§€ ì½ê¸°
        for _ in 0..count {
            let message_size = u32::decode(&mut cursor)? as usize;
            let start_pos = cursor.position() as usize;
            let end_pos = start_pos + message_size;
            
            if end_pos > data.len() {
                return Err(DecodeError::new("Unexpected end of input"));
            }
            
            let message_data = &data[start_pos..end_pos];
            let message = T::decode(message_data)?;
            messages.push(message);
            
            cursor.set_position(end_pos as u64);
        }
        
        Ok(messages)
    }
    
    /// ë°ì´í„° ì••ì¶•
    fn compress_data(&self, data: &[u8]) -> Result<Vec<u8>, EncodeError> {
        // LZ4 ê³ ì† ì••ì¶• ì‚¬ìš©
        let compressed = lz4_flex::compress_prepend_size(data);
        Ok(compressed)
    }
    
    /// ë°ì´í„° ì••ì¶• í•´ì œ
    fn decompress_data(&self, data: &[u8]) -> Result<Vec<u8>, DecodeError> {
        lz4_flex::decompress_size_prepended(data)
            .map_err(|e| DecodeError::new(format!("Decompression failed: {}", e)))
    }
    
    /// ì••ì¶• ì—¬ë¶€ í™•ì¸
    fn is_compressed(&self, data: &[u8]) -> bool {
        data.len() > 4 && &data[0..4] == b"LZ4\x01" // LZ4 ë§¤ì§ ë°”ì´íŠ¸
    }
}

/// Proto ë©”ì‹œì§€ ìºì‹œ
struct ProtoCache {
    cache: Arc<std::sync::RwLock<lru::LruCache<Vec<u8>, CachedMessage>>>,
}

#[derive(Clone)]
struct CachedMessage {
    data: Vec<u8>,
    last_access: std::time::Instant,
}

impl ProtoCache {
    fn new() -> Self {
        Self {
            cache: Arc::new(std::sync::RwLock::new(lru::LruCache::new(1000.try_into().unwrap()))),
        }
    }
    
    fn get<T: Message + Default>(&self, key: &[u8]) -> Option<T> {
        let mut cache = self.cache.write().unwrap();
        
        if let Some(cached) = cache.get_mut(key) {
            cached.last_access = std::time::Instant::now();
            T::decode(&*cached.data).ok()
        } else {
            None
        }
    }
    
    fn put<T: Message>(&self, key: &[u8], message: T) {
        let mut buffer = Vec::new();
        if message.encode(&mut buffer).is_ok() {
            let mut cache = self.cache.write().unwrap();
            cache.put(
                key.to_vec(),
                CachedMessage {
                    data: buffer,
                    last_access: std::time::Instant::now(),
                },
            );
        }
    }
}
```

## ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### 1. ì—°ê²° í’€ ìµœì í™”

```rust
// src/database/optimized_pool.rs
use sqlx::{Pool, MySql, MySqlPool, Row};
use std::time::Duration;

pub struct OptimizedDatabasePool {
    read_pool: MySqlPool,
    write_pool: MySqlPool,
    cache: Arc<QueryCache>,
    metrics: Arc<DatabaseMetrics>,
}

#[derive(Debug, Clone)]
pub struct DatabaseConfig {
    pub read_url: String,
    pub write_url: String,
    pub max_read_connections: u32,
    pub max_write_connections: u32,
    pub connection_timeout: Duration,
    pub idle_timeout: Duration,
    pub max_lifetime: Duration,
    pub enable_query_cache: bool,
    pub cache_size: usize,
}

impl OptimizedDatabasePool {
    pub async fn new(config: DatabaseConfig) -> Result<Self, sqlx::Error> {
        // ì½ê¸° ì „ìš© í’€ ì„¤ì •
        let read_pool = MySqlPool::connect_with(
            sqlx::mysql::MySqlConnectOptions::from_url(&config.read_url.parse().unwrap())?
                .statement_cache_capacity(100)  // ì¤€ë¹„ëœ ë¬¸ì¥ ìºì‹œ
        )
        .max_connections(config.max_read_connections)
        .acquire_timeout(config.connection_timeout)
        .idle_timeout(Some(config.idle_timeout))
        .max_lifetime(Some(config.max_lifetime))
        .build()
        .await?;
        
        // ì“°ê¸° ì „ìš© í’€ ì„¤ì •
        let write_pool = MySqlPool::connect_with(
            sqlx::mysql::MySqlConnectOptions::from_url(&config.write_url.parse().unwrap())?
                .statement_cache_capacity(50)
        )
        .max_connections(config.max_write_connections)
        .acquire_timeout(config.connection_timeout)
        .idle_timeout(Some(config.idle_timeout))
        .max_lifetime(Some(config.max_lifetime))
        .build()
        .await?;
        
        Ok(Self {
            read_pool,
            write_pool,
            cache: Arc::new(QueryCache::new(config.cache_size)),
            metrics: Arc::new(DatabaseMetrics::new()),
        })
    }
    
    /// ìµœì í™”ëœ SELECT ì¿¼ë¦¬
    pub async fn execute_read_query<T>(
        &self,
        query: &str,
        params: &[&(dyn sqlx::Encode<'_, MySql> + sqlx::Type<MySql> + Sync)],
    ) -> Result<Vec<T>, sqlx::Error>
    where
        T: for<'r> sqlx::FromRow<'r, sqlx::mysql::MySqlRow> + Send + Unpin,
    {
        let start_time = std::time::Instant::now();
        
        // ìºì‹œ í™•ì¸
        let cache_key = self.generate_cache_key(query, params);
        if let Some(cached_result) = self.cache.get::<Vec<T>>(&cache_key) {
            self.metrics.record_cache_hit(start_time.elapsed());
            return Ok(cached_result);
        }
        
        // ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰
        let mut query_builder = sqlx::query_as::<_, T>(query);
        for param in params {
            query_builder = query_builder.bind(*param);
        }
        
        let result = query_builder
            .fetch_all(&self.read_pool)
            .await;
        
        match result {
            Ok(data) => {
                // ìºì‹œì— ì €ì¥ (ì‘ì€ ê²°ê³¼ë§Œ)
                if data.len() < 1000 {
                    self.cache.put(cache_key, data.clone(), Duration::from_secs(300));
                }
                
                self.metrics.record_query_success(start_time.elapsed());
                Ok(data)
            }
            Err(e) => {
                self.metrics.record_query_error(start_time.elapsed());
                Err(e)
            }
        }
    }
    
    /// ìµœì í™”ëœ INSERT/UPDATE/DELETE ì¿¼ë¦¬
    pub async fn execute_write_query(
        &self,
        query: &str,
        params: &[&(dyn sqlx::Encode<'_, MySql> + sqlx::Type<MySql> + Sync)],
    ) -> Result<sqlx::mysql::MySqlQueryResult, sqlx::Error> {
        let start_time = std::time::Instant::now();
        
        let mut query_builder = sqlx::query(query);
        for param in params {
            query_builder = query_builder.bind(*param);
        }
        
        let result = query_builder
            .execute(&self.write_pool)
            .await;
        
        match result {
            Ok(result) => {
                // ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
                self.invalidate_related_cache(query);
                
                self.metrics.record_write_success(start_time.elapsed());
                Ok(result)
            }
            Err(e) => {
                self.metrics.record_write_error(start_time.elapsed());
                Err(e)
            }
        }
    }
    
    /// ë°°ì¹˜ ì‚½ì… ìµœì í™”
    pub async fn batch_insert<T>(
        &self,
        table: &str,
        columns: &[&str],
        data: &[T],
    ) -> Result<u64, sqlx::Error>
    where
        T: BatchInsertable,
    {
        let start_time = std::time::Instant::now();
        
        if data.is_empty() {
            return Ok(0);
        }
        
        // VALUES ì ˆì„ ë™ì ìœ¼ë¡œ ìƒì„±
        let placeholders: Vec<String> = data.iter()
            .map(|_| format!("({})", columns.iter().map(|_| "?").collect::<Vec<_>>().join(", ")))
            .collect();
        
        let query = format!(
            "INSERT INTO {} ({}) VALUES {}",
            table,
            columns.join(", "),
            placeholders.join(", ")
        );
        
        let mut query_builder = sqlx::query(&query);
        
        // ëª¨ë“  íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
        for item in data {
            let values = item.get_values();
            for value in values {
                query_builder = query_builder.bind(value);
            }
        }
        
        let result = query_builder
            .execute(&self.write_pool)
            .await?;
        
        self.metrics.record_batch_insert(data.len(), start_time.elapsed());
        Ok(result.rows_affected())
    }
    
    /// íŠ¸ëœì­ì…˜ ìµœì í™”
    pub async fn execute_transaction<F, R>(&self, callback: F) -> Result<R, sqlx::Error>
    where
        F: for<'c> FnOnce(&'c mut sqlx::Transaction<'_, MySql>) -> std::pin::Pin<Box<dyn std::future::Future<Output = Result<R, sqlx::Error>> + Send + 'c>> + Send,
        R: Send,
    {
        let start_time = std::time::Instant::now();
        let mut tx = self.write_pool.begin().await?;
        
        match callback(&mut tx).await {
            Ok(result) => {
                tx.commit().await?;
                self.metrics.record_transaction_success(start_time.elapsed());
                Ok(result)
            }
            Err(e) => {
                tx.rollback().await?;
                self.metrics.record_transaction_error(start_time.elapsed());
                Err(e)
            }
        }
    }
    
    /// ìºì‹œ í‚¤ ìƒì„±
    fn generate_cache_key(
        &self,
        query: &str,
        params: &[&(dyn sqlx::Encode<'_, MySql> + sqlx::Type<MySql> + Sync)],
    ) -> String {
        use std::hash::{Hash, Hasher};
        let mut hasher = std::collections::hash_map::DefaultHasher::new();
        query.hash(&mut hasher);
        // íŒŒë¼ë¯¸í„°ëŠ” í•´ì‹œí•˜ê¸° ë³µì¡í•˜ë¯€ë¡œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
        format!("query:{:x}", hasher.finish())
    }
    
    /// ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
    fn invalidate_related_cache(&self, query: &str) {
        // INSERT/UPDATE/DELETEì— ì˜í–¥ë°›ëŠ” í…Œì´ë¸” ì¶”ì¶œ
        if let Some(table) = self.extract_table_name(query) {
            self.cache.invalidate_by_pattern(&format!("table:{}", table));
        }
    }
    
    /// ì¿¼ë¦¬ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
    fn extract_table_name(&self, query: &str) -> Option<String> {
        let query = query.to_uppercase();
        
        if let Some(pos) = query.find("FROM ") {
            let after_from = &query[pos + 5..];
            if let Some(end) = after_from.find(' ') {
                Some(after_from[..end].trim().to_string())
            } else {
                Some(after_from.trim().to_string())
            }
        } else if let Some(pos) = query.find("UPDATE ") {
            let after_update = &query[pos + 7..];
            if let Some(end) = after_update.find(' ') {
                Some(after_update[..end].trim().to_string())
            } else {
                Some(after_update.trim().to_string())
            }
        } else if let Some(pos) = query.find("INSERT INTO ") {
            let after_insert = &query[pos + 12..];
            if let Some(end) = after_insert.find(' ') {
                Some(after_insert[..end].trim().to_string())
            } else {
                Some(after_insert.trim().to_string())
            }
        } else {
            None
        }
    }
    
    /// ì—°ê²° í’€ ìƒíƒœ ì¡°íšŒ
    pub fn get_pool_status(&self) -> PoolStatus {
        PoolStatus {
            read_pool_size: self.read_pool.size(),
            read_pool_idle: self.read_pool.num_idle(),
            write_pool_size: self.write_pool.size(),
            write_pool_idle: self.write_pool.num_idle(),
            cache_hit_rate: self.cache.get_hit_rate(),
        }
    }
}

pub trait BatchInsertable {
    fn get_values(&self) -> Vec<&(dyn sqlx::Encode<'_, MySql> + sqlx::Type<MySql> + Sync)>;
}

#[derive(Debug)]
pub struct PoolStatus {
    pub read_pool_size: u32,
    pub read_pool_idle: u32,
    pub write_pool_size: u32,
    pub write_pool_idle: u32,
    pub cache_hit_rate: f64,
}
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

### 1. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```rust
// src/monitoring/performance_monitor.rs
use prometheus::{Counter, Histogram, Gauge, Registry};
use std::sync::Arc;
use std::time::{Duration, Instant};

pub struct PerformanceMonitor {
    registry: Registry,
    
    // gRPC ë©”íŠ¸ë¦­
    grpc_requests_total: Counter,
    grpc_request_duration: Histogram,
    grpc_active_connections: Gauge,
    
    // ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­
    db_queries_total: Counter,
    db_query_duration: Histogram,
    db_connections_active: Gauge,
    
    // ìºì‹œ ë©”íŠ¸ë¦­
    cache_hits_total: Counter,
    cache_misses_total: Counter,
    cache_evictions_total: Counter,
    
    // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
    memory_usage_bytes: Gauge,
    cpu_usage_percent: Gauge,
    goroutines_count: Gauge,
}

impl PerformanceMonitor {
    pub fn new() -> Self {
        let registry = Registry::new();
        
        let grpc_requests_total = Counter::new(
            "grpc_requests_total", 
            "Total number of gRPC requests"
        ).unwrap();
        
        let grpc_request_duration = Histogram::with_opts(
            prometheus::HistogramOpts::new(
                "grpc_request_duration_seconds",
                "Duration of gRPC requests in seconds"
            ).buckets(vec![0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0])
        ).unwrap();
        
        let grpc_active_connections = Gauge::new(
            "grpc_active_connections",
            "Number of active gRPC connections"
        ).unwrap();
        
        let db_queries_total = Counter::new(
            "db_queries_total",
            "Total number of database queries"
        ).unwrap();
        
        let db_query_duration = Histogram::with_opts(
            prometheus::HistogramOpts::new(
                "db_query_duration_seconds",
                "Duration of database queries in seconds"
            ).buckets(vec![0.0001, 0.001, 0.01, 0.1, 1.0])
        ).unwrap();
        
        let db_connections_active = Gauge::new(
            "db_connections_active",
            "Number of active database connections"
        ).unwrap();
        
        let cache_hits_total = Counter::new(
            "cache_hits_total",
            "Total number of cache hits"
        ).unwrap();
        
        let cache_misses_total = Counter::new(
            "cache_misses_total",
            "Total number of cache misses"
        ).unwrap();
        
        let cache_evictions_total = Counter::new(
            "cache_evictions_total",
            "Total number of cache evictions"
        ).unwrap();
        
        let memory_usage_bytes = Gauge::new(
            "memory_usage_bytes",
            "Memory usage in bytes"
        ).unwrap();
        
        let cpu_usage_percent = Gauge::new(
            "cpu_usage_percent",
            "CPU usage percentage"
        ).unwrap();
        
        let goroutines_count = Gauge::new(
            "goroutines_count",
            "Number of goroutines"
        ).unwrap();
        
        // ë©”íŠ¸ë¦­ ë“±ë¡
        registry.register(Box::new(grpc_requests_total.clone())).unwrap();
        registry.register(Box::new(grpc_request_duration.clone())).unwrap();
        registry.register(Box::new(grpc_active_connections.clone())).unwrap();
        registry.register(Box::new(db_queries_total.clone())).unwrap();
        registry.register(Box::new(db_query_duration.clone())).unwrap();
        registry.register(Box::new(db_connections_active.clone())).unwrap();
        registry.register(Box::new(cache_hits_total.clone())).unwrap();
        registry.register(Box::new(cache_misses_total.clone())).unwrap();
        registry.register(Box::new(cache_evictions_total.clone())).unwrap();
        registry.register(Box::new(memory_usage_bytes.clone())).unwrap();
        registry.register(Box::new(cpu_usage_percent.clone())).unwrap();
        registry.register(Box::new(goroutines_count.clone())).unwrap();
        
        Self {
            registry,
            grpc_requests_total,
            grpc_request_duration,
            grpc_active_connections,
            db_queries_total,
            db_query_duration,
            db_connections_active,
            cache_hits_total,
            cache_misses_total,
            cache_evictions_total,
            memory_usage_bytes,
            cpu_usage_percent,
            goroutines_count,
        }
    }
    
    /// gRPC ìš”ì²­ ê¸°ë¡
    pub fn record_grpc_request(&self, method: &str, duration: Duration, success: bool) {
        self.grpc_requests_total.inc();
        self.grpc_request_duration.observe(duration.as_secs_f64());
        
        tracing::info!(
            method = method,
            duration_ms = duration.as_millis(),
            success = success,
            "gRPC request completed"
        );
    }
    
    /// ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ê¸°ë¡
    pub fn record_db_query(&self, query_type: &str, duration: Duration, success: bool) {
        self.db_queries_total.inc();
        self.db_query_duration.observe(duration.as_secs_f64());
        
        if !success {
            tracing::warn!(
                query_type = query_type,
                duration_ms = duration.as_millis(),
                "Database query failed"
            );
        }
    }
    
    /// ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ê¸°ë¡
    pub fn record_cache_hit(&self) {
        self.cache_hits_total.inc();
    }
    
    pub fn record_cache_miss(&self) {
        self.cache_misses_total.inc();
    }
    
    pub fn record_cache_eviction(&self) {
        self.cache_evictions_total.inc();
    }
    
    /// ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    pub async fn update_system_metrics(&self) {
        // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        let memory_usage = self.get_memory_usage().await;
        self.memory_usage_bytes.set(memory_usage);
        
        // CPU ì‚¬ìš©ë¥  ì¸¡ì •
        let cpu_usage = self.get_cpu_usage().await;
        self.cpu_usage_percent.set(cpu_usage);
        
        // ê³ ë£¨í‹´ ìˆ˜ (Tokio íƒœìŠ¤í¬ ìˆ˜ë¡œ ëŒ€ì²´)
        let task_count = self.get_active_task_count().await;
        self.goroutines_count.set(task_count);
    }
    
    /// Prometheus ë©”íŠ¸ë¦­ ìµìŠ¤í¬íŠ¸
    pub fn export_metrics(&self) -> String {
        let encoder = prometheus::TextEncoder::new();
        let metric_families = self.registry.gather();
        encoder.encode_to_string(&metric_families).unwrap_or_default()
    }
    
    /// ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    pub fn generate_performance_report(&self) -> PerformanceReport {
        PerformanceReport {
            total_grpc_requests: self.grpc_requests_total.get(),
            avg_grpc_duration_ms: self.get_avg_grpc_duration(),
            cache_hit_rate: self.calculate_cache_hit_rate(),
            active_connections: self.grpc_active_connections.get() as u32,
            memory_usage_mb: self.memory_usage_bytes.get() / 1024.0 / 1024.0,
            cpu_usage_percent: self.cpu_usage_percent.get(),
            generated_at: chrono::Utc::now(),
        }
    }
    
    async fn get_memory_usage(&self) -> f64 {
        // í”Œë«í¼ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        #[cfg(target_os = "linux")]
        {
            if let Ok(contents) = tokio::fs::read_to_string("/proc/self/status").await {
                for line in contents.lines() {
                    if line.starts_with("VmRSS:") {
                        if let Ok(kb) = line.split_whitespace().nth(1).unwrap_or("0").parse::<f64>() {
                            return kb * 1024.0; // KB to bytes
                        }
                    }
                }
            }
        }
        
        0.0 // ì¸¡ì • ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
    }
    
    async fn get_cpu_usage(&self) -> f64 {
        // ê°„ë‹¨í•œ CPU ì‚¬ìš©ë¥  ì¸¡ì • (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ì¸¡ì • í•„ìš”)
        50.0 // ì„ì‹œê°’
    }
    
    async fn get_active_task_count(&self) -> f64 {
        // Tokio ëŸ°íƒ€ì„ì˜ í™œì„± íƒœìŠ¤í¬ ìˆ˜ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
        100.0 // ì„ì‹œê°’
    }
    
    fn get_avg_grpc_duration(&self) -> f64 {
        // Histogramì—ì„œ í‰ê·  ê³„ì‚°
        let sum = self.grpc_request_duration.get_sample_sum();
        let count = self.grpc_request_duration.get_sample_count();
        
        if count > 0 {
            (sum / count as f64) * 1000.0 // seconds to milliseconds
        } else {
            0.0
        }
    }
    
    fn calculate_cache_hit_rate(&self) -> f64 {
        let hits = self.cache_hits_total.get();
        let misses = self.cache_misses_total.get();
        let total = hits + misses;
        
        if total > 0 {
            hits / total
        } else {
            0.0
        }
    }
}

#[derive(Debug, Clone)]
pub struct PerformanceReport {
    pub total_grpc_requests: f64,
    pub avg_grpc_duration_ms: f64,
    pub cache_hit_rate: f64,
    pub active_connections: u32,
    pub memory_usage_mb: f64,
    pub cpu_usage_percent: f64,
    pub generated_at: chrono::DateTime<chrono::Utc>,
}

impl PerformanceReport {
    pub fn to_string(&self) -> String {
        format!(
            "gRPC ì„œë²„ ì„±ëŠ¥ ë³´ê³ ì„œ ({})\n\
            â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\
            ğŸ“Š ìš”ì²­ í†µê³„:\n\
            â€¢ ì´ ìš”ì²­ ìˆ˜: {:.0}\n\
            â€¢ í‰ê·  ì‘ë‹µ ì‹œê°„: {:.2}ms\n\
            â€¢ í™œì„± ì—°ê²° ìˆ˜: {}\n\
            \n\
            ğŸ’¾ ìºì‹œ ì„±ëŠ¥:\n\
            â€¢ ìºì‹œ íˆíŠ¸ìœ¨: {:.2}%\n\
            \n\
            ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:\n\
            â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {:.1}MB\n\
            â€¢ CPU ì‚¬ìš©ë¥ : {:.1}%\n\
            \n\
            ğŸ“… ìƒì„± ì‹œê°„: {}",
            self.generated_at.format("%Y-%m-%d %H:%M:%S UTC"),
            self.total_grpc_requests,
            self.avg_grpc_duration_ms,
            self.active_connections,
            self.cache_hit_rate * 100.0,
            self.memory_usage_mb,
            self.cpu_usage_percent,
            self.generated_at.format("%Y-%m-%d %H:%M:%S UTC")
        )
    }
}
```

ì´ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œëŠ” gRPC ì„œë²„ì˜ ëª¨ë“  ê³„ì¸µì—ì„œ ìµœì í™”ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. HTTP/2 ë©€í‹°í”Œë ‰ì‹± í™œìš©, Protocol Buffers ìµœì í™”, ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ë§, ì§€ëŠ¥í˜• ìºì‹±, ê·¸ë¦¬ê³  í¬ê´„ì ì¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ë†’ì€ ì„±ëŠ¥ê³¼ í™•ì¥ì„±ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.