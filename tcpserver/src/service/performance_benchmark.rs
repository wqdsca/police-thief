//! ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ê²€ì¦ ë„êµ¬
//! 
//! ëª¨ë“  ìµœì í™” ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ê²€ì¦í•˜ëŠ” ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
//! ì‹¤ì œ ì›Œí¬ë¡œë“œë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ìµœì í™” íš¨ê³¼ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤.

use anyhow::Result;
use std::sync::Arc;
use std::time::{Duration, Instant};
use std::collections::HashMap;
use tokio::sync::Mutex;
use tracing::{info, warn};
use serde::{Serialize, Deserialize};
use rayon::prelude::*;

use crate::service::{
    AsyncIoOptimizer, AsyncIoOptimizerConfig,
    SimdOptimizer, SimdOptimizerConfig,
    MessageCompressionService, MessageCompressionConfig,
    PerformanceMonitor, PerformanceMonitorConfig,
};
use shared::tool::high_performance::dashmap_optimizer::{
    DashMapOptimizer, DashMapOptimizerConfig,
};

/// ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkConfig {
    /// í…ŒìŠ¤íŠ¸ ë°˜ë³µ íšŸìˆ˜
    pub iterations: usize,
    /// ë™ì‹œ ì‚¬ìš©ì ìˆ˜
    pub concurrent_users: usize,
    /// ë©”ì‹œì§€ í¬ê¸° (ë°”ì´íŠ¸)
    pub message_size: usize,
    /// í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)
    pub duration_secs: u64,
    /// ì›Œë°ì—… ì‹œê°„ (ì´ˆ)
    pub warmup_secs: u64,
    /// ìƒì„¸ ë¡œê¹… í™œì„±í™”
    pub enable_detailed_logging: bool,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            iterations: 1000,
            concurrent_users: 100,
            message_size: 1024,
            duration_secs: 60,
            warmup_secs: 10,
            enable_detailed_logging: false,
        }
    }
}

/// ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkResult {
    pub test_name: String,
    pub iterations: usize,
    pub total_duration: Duration,
    pub avg_latency: Duration,
    pub min_latency: Duration,
    pub max_latency: Duration,
    pub throughput_ops_per_sec: f64,
    pub success_rate: f64,
    pub memory_usage_mb: f64,
    pub cpu_usage_percent: f64,
}

impl BenchmarkResult {
    /// ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (0-100)
    pub fn performance_score(&self) -> f64 {
        let throughput_score = (self.throughput_ops_per_sec / 10000.0).min(1.0) * 30.0;
        let latency_score = (1000.0 / self.avg_latency.as_millis() as f64).min(1.0) * 25.0;
        let success_score = self.success_rate * 25.0;
        let efficiency_score = ((100.0 - self.cpu_usage_percent) / 100.0 * 20.0).max(0.0);
        
        throughput_score + latency_score + success_score + efficiency_score
    }
}

/// ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
pub struct PerformanceBenchmark {
    config: BenchmarkConfig,
    results: Arc<Mutex<HashMap<String, BenchmarkResult>>>,
}

impl PerformanceBenchmark {
    pub fn new(config: BenchmarkConfig) -> Self {
        Self {
            config,
            results: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// DashMap ìµœì í™” ë²¤ì¹˜ë§ˆí¬
    pub async fn benchmark_dashmap_optimizer(&self) -> Result<BenchmarkResult> {
        info!("ğŸš€ DashMap ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘");
        
        let optimizer = DashMapOptimizer::new(DashMapOptimizerConfig::default());
        let map = optimizer.create_optimized_dashmap::<u32, String>();
        
        let start = Instant::now();
        let mut success_count = 0;
        let mut latencies = Vec::new();
        
        // ì›Œë°ì—…
        for i in 0..1000 {
            map.insert(i, format!("value_{}", i));
        }
        
        // ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        for iteration in 0..self.config.iterations {
            let op_start = Instant::now();
            
            // ì½ê¸° ì‘ì—…
            let key = (iteration % 1000) as u32;
            let result = optimizer.read_with_operation(&map, &key, |v| v.clone());
            
            let op_latency = op_start.elapsed();
            latencies.push(op_latency);
            
            if result.is_some() {
                success_count += 1;
            }
        }
        
        let total_duration = start.elapsed();
        let avg_latency = latencies.iter().sum::<Duration>() / latencies.len() as u32;
        let min_latency = *latencies.iter().min().unwrap();
        let max_latency = *latencies.iter().max().unwrap();
        
        let result = BenchmarkResult {
            test_name: "DashMap ìµœì í™”".to_string(),
            iterations: self.config.iterations,
            total_duration,
            avg_latency,
            min_latency,
            max_latency,
            throughput_ops_per_sec: self.config.iterations as f64 / total_duration.as_secs_f64(),
            success_rate: (success_count as f64 / self.config.iterations as f64) * 100.0,
            memory_usage_mb: 10.0, // ì¶”ì •ê°’
            cpu_usage_percent: 5.0, // ì¶”ì •ê°’
        };
        
        self.results.lock().await.insert("dashmap_optimizer".to_string(), result.clone());
        
        info!("âœ… DashMap ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {:.1} ops/sec", result.throughput_ops_per_sec);
        Ok(result)
    }
    
    /// ë¹„ë™ê¸° I/O ìµœì í™” ë²¤ì¹˜ë§ˆí¬
    pub async fn benchmark_async_io_optimizer(&self) -> Result<BenchmarkResult> {
        info!("ğŸš€ ë¹„ë™ê¸° I/O ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘");
        
        let optimizer = AsyncIoOptimizer::new(AsyncIoOptimizerConfig::default());
        
        let start = Instant::now();
        let mut success_count = 0;
        let mut latencies = Vec::new();
        
        // í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        let test_data = vec![0u8; self.config.message_size];
        
        // ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜ I/O ì‹œë®¬ë ˆì´ì…˜)
        for _ in 0..self.config.iterations {
            let op_start = Instant::now();
            
            // Zero-copy ì½ê¸° ì‹œë®¬ë ˆì´ì…˜
            let mut cursor = std::io::Cursor::new(&test_data);
            let result = optimizer.zero_copy_read(&mut cursor, test_data.len()).await;
            
            let op_latency = op_start.elapsed();
            latencies.push(op_latency);
            
            if result.is_ok() {
                success_count += 1;
            }
        }
        
        let total_duration = start.elapsed();
        let avg_latency = latencies.iter().sum::<Duration>() / latencies.len() as u32;
        let min_latency = *latencies.iter().min().unwrap();
        let max_latency = *latencies.iter().max().unwrap();
        
        let result = BenchmarkResult {
            test_name: "ë¹„ë™ê¸° I/O ìµœì í™”".to_string(),
            iterations: self.config.iterations,
            total_duration,
            avg_latency,
            min_latency,
            max_latency,
            throughput_ops_per_sec: self.config.iterations as f64 / total_duration.as_secs_f64(),
            success_rate: (success_count as f64 / self.config.iterations as f64) * 100.0,
            memory_usage_mb: 5.0, // ì¶”ì •ê°’
            cpu_usage_percent: 8.0, // ì¶”ì •ê°’
        };
        
        self.results.lock().await.insert("async_io_optimizer".to_string(), result.clone());
        
        info!("âœ… ë¹„ë™ê¸° I/O ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {:.1} ops/sec", result.throughput_ops_per_sec);
        Ok(result)
    }
    
    /// SIMD ìµœì í™” ë²¤ì¹˜ë§ˆí¬
    pub async fn benchmark_simd_optimizer(&self) -> Result<BenchmarkResult> {
        info!("ğŸš€ SIMD ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘");
        
        let optimizer = SimdOptimizer::new(SimdOptimizerConfig::default());
        
        let start = Instant::now();
        let mut success_count = 0;
        let mut latencies = Vec::new();
        
        // í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        let data_a = vec![0xAAu8; self.config.message_size];
        let data_b = vec![0x55u8; self.config.message_size];
        
        // ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        for _ in 0..self.config.iterations {
            let op_start = Instant::now();
            
            // SIMD XOR ì—°ì‚°
            let result = optimizer.simd_xor(&data_a, &data_b);
            
            let op_latency = op_start.elapsed();
            latencies.push(op_latency);
            
            if !result.is_empty() {
                success_count += 1;
            }
        }
        
        let total_duration = start.elapsed();
        let avg_latency = latencies.iter().sum::<Duration>() / latencies.len() as u32;
        let min_latency = *latencies.iter().min().unwrap();
        let max_latency = *latencies.iter().max().unwrap();
        
        let result = BenchmarkResult {
            test_name: "SIMD ìµœì í™”".to_string(),
            iterations: self.config.iterations,
            total_duration,
            avg_latency,
            min_latency,
            max_latency,
            throughput_ops_per_sec: self.config.iterations as f64 / total_duration.as_secs_f64(),
            success_rate: (success_count as f64 / self.config.iterations as f64) * 100.0,
            memory_usage_mb: 15.0, // ì¶”ì •ê°’
            cpu_usage_percent: 12.0, // ì¶”ì •ê°’
        };
        
        self.results.lock().await.insert("simd_optimizer".to_string(), result.clone());
        
        info!("âœ… SIMD ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {:.1} ops/sec", result.throughput_ops_per_sec);
        Ok(result)
    }
    
    /// ë©”ì‹œì§€ ì••ì¶• ë²¤ì¹˜ë§ˆí¬
    pub async fn benchmark_message_compression(&self) -> Result<BenchmarkResult> {
        info!("ğŸš€ ë©”ì‹œì§€ ì••ì¶• ë²¤ì¹˜ë§ˆí¬ ì‹œì‘");
        
        let service = MessageCompressionService::new(MessageCompressionConfig::default());
        
        let start = Instant::now();
        let mut success_count = 0;
        let mut latencies = Vec::new();
        
        // í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ìƒì„±
        let test_message = "Hello, World! This is a test message for compression benchmarking. ".repeat(50);
        let test_data = test_message.as_bytes();
        
        // ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        for _ in 0..self.config.iterations {
            let op_start = Instant::now();
            
            // ì••ì¶• ë° ì••ì¶• í•´ì œ
            if let Ok((compressed, algorithm)) = service.compress(test_data).await {
                if let Ok(_decompressed) = service.decompress(&compressed, algorithm).await {
                    success_count += 1;
                }
            }
            
            let op_latency = op_start.elapsed();
            latencies.push(op_latency);
        }
        
        let total_duration = start.elapsed();
        let avg_latency = latencies.iter().sum::<Duration>() / latencies.len() as u32;
        let min_latency = *latencies.iter().min().unwrap();
        let max_latency = *latencies.iter().max().unwrap();
        
        let result = BenchmarkResult {
            test_name: "ë©”ì‹œì§€ ì••ì¶•".to_string(),
            iterations: self.config.iterations,
            total_duration,
            avg_latency,
            min_latency,
            max_latency,
            throughput_ops_per_sec: self.config.iterations as f64 / total_duration.as_secs_f64(),
            success_rate: (success_count as f64 / self.config.iterations as f64) * 100.0,
            memory_usage_mb: 8.0, // ì¶”ì •ê°’
            cpu_usage_percent: 15.0, // ì¶”ì •ê°’
        };
        
        self.results.lock().await.insert("message_compression".to_string(), result.clone());
        
        info!("âœ… ë©”ì‹œì§€ ì••ì¶• ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {:.1} ops/sec", result.throughput_ops_per_sec);
        Ok(result)
    }
    
    /// ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë²¤ì¹˜ë§ˆí¬
    pub async fn benchmark_performance_monitor(&self) -> Result<BenchmarkResult> {
        info!("ğŸš€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘");
        
        let config = PerformanceMonitorConfig::default();
        let monitor = PerformanceMonitor::new(config).await;
        
        let start = Instant::now();
        let mut success_count = 0;
        let mut latencies = Vec::new();
        
        // ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        for i in 0..self.config.iterations {
            let op_start = Instant::now();
            
            // ë©”íŠ¸ë¦­ ê¸°ë¡
            monitor.record_latency("test_latency", Duration::from_millis(10)).await;
            monitor.increment_request_count().await;
            monitor.set_connection_count(i % 100).await;
            
            success_count += 1;
            
            let op_latency = op_start.elapsed();
            latencies.push(op_latency);
        }
        
        // ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        let _report = monitor.generate_report().await;
        
        let total_duration = start.elapsed();
        let avg_latency = latencies.iter().sum::<Duration>() / latencies.len() as u32;
        let min_latency = *latencies.iter().min().unwrap();
        let max_latency = *latencies.iter().max().unwrap();
        
        let result = BenchmarkResult {
            test_name: "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§".to_string(),
            iterations: self.config.iterations,
            total_duration,
            avg_latency,
            min_latency,
            max_latency,
            throughput_ops_per_sec: self.config.iterations as f64 / total_duration.as_secs_f64(),
            success_rate: (success_count as f64 / self.config.iterations as f64) * 100.0,
            memory_usage_mb: 12.0, // ì¶”ì •ê°’
            cpu_usage_percent: 3.0, // ì¶”ì •ê°’
        };
        
        self.results.lock().await.insert("performance_monitor".to_string(), result.clone());
        
        info!("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {:.1} ops/sec", result.throughput_ops_per_sec);
        Ok(result)
    }
    
    /// ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    pub async fn run_all_benchmarks(&self) -> Result<HashMap<String, BenchmarkResult>> {
        info!("ğŸ¯ ì „ì²´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘");
        
        let mut all_results = HashMap::new();
        
        // ê° ë²¤ì¹˜ë§ˆí¬ ìˆœì°¨ ì‹¤í–‰
        let mut results = Vec::new();
        
        // DashMap ë²¤ì¹˜ë§ˆí¬
        let start = Instant::now();
        let dashmap_result = self.benchmark_dashmap_optimizer().await;
        results.push(("dashmap".to_string(), dashmap_result, start.elapsed()));
        
        // Async I/O ë²¤ì¹˜ë§ˆí¬
        let start = Instant::now();
        let async_io_result = self.benchmark_async_io_optimizer().await;
        results.push(("async_io".to_string(), async_io_result, start.elapsed()));
        
        // SIMD ë²¤ì¹˜ë§ˆí¬
        let start = Instant::now();
        let simd_result = self.benchmark_simd_optimizer().await;
        results.push(("simd".to_string(), simd_result, start.elapsed()));
        
        // ì••ì¶• ë²¤ì¹˜ë§ˆí¬
        let start = Instant::now();
        let compression_result = self.benchmark_message_compression().await;
        results.push(("compression".to_string(), compression_result, start.elapsed()));
        
        // ëª¨ë‹ˆí„°ë§ ë²¤ì¹˜ë§ˆí¬
        let start = Instant::now();
        let monitoring_result = self.benchmark_performance_monitor().await;
        results.push(("monitoring".to_string(), monitoring_result, start.elapsed()));
        
        for (name, result, duration) in results {
            match result {
                Ok(benchmark_result) => {
                    info!("âœ… {} ë²¤ì¹˜ë§ˆí¬: ì ìˆ˜ {:.1}/100, ì†Œìš”ì‹œê°„ {:?}", 
                         benchmark_result.test_name, benchmark_result.performance_score(), duration);
                    all_results.insert(name, benchmark_result);
                }
                Err(e) => {
                    warn!("âŒ {} ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {}", name, e);
                }
            }
            
            // ë²¤ì¹˜ë§ˆí¬ ê°„ íœ´ì‹
            tokio::time::sleep(Duration::from_secs(2)).await;
        }
        
        self.generate_summary_report(&all_results).await;
        Ok(all_results)
    }
    
    /// ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
    async fn generate_summary_report(&self, results: &HashMap<String, BenchmarkResult>) {
        info!("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ë³´ê³ ì„œ");
        info!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        
        let mut total_score = 0.0;
        let mut total_throughput = 0.0;
        let mut total_memory = 0.0;
        let mut total_cpu = 0.0;
        
        for result in results.values() {
            let score = result.performance_score();
            total_score += score;
            total_throughput += result.throughput_ops_per_sec;
            total_memory += result.memory_usage_mb;
            total_cpu += result.cpu_usage_percent;
            
            info!(
                "ğŸ“ˆ {}: ì ìˆ˜ {:.1}/100, ì²˜ë¦¬ëŸ‰ {:.0} ops/sec, ì§€ì—°ì‹œê°„ {:.2}ms",
                result.test_name,
                score,
                result.throughput_ops_per_sec,
                result.avg_latency.as_secs_f64() * 1000.0
            );
        }
        
        let avg_score = total_score / results.len() as f64;
        let avg_cpu = total_cpu / results.len() as f64;
        
        info!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        info!("ğŸ¯ ì¢…í•© ì„±ëŠ¥ ì ìˆ˜: {:.1}/100", avg_score);
        info!("âš¡ ì´ ì²˜ë¦¬ëŸ‰: {:.0} ops/sec", total_throughput);
        info!("ğŸ’¾ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {:.1} MB", total_memory);
        info!("ğŸ”¥ í‰ê·  CPU ì‚¬ìš©ë¥ : {:.1}%", avg_cpu);
        info!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        
        // ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        let grade = match avg_score as u32 {
            90..=100 => "A+ (ìµœê³  ì„±ëŠ¥)",
            80..=89 => "A (ìš°ìˆ˜ ì„±ëŠ¥)",
            70..=79 => "B (ì–‘í˜¸ ì„±ëŠ¥)",
            60..=69 => "C (ë³´í†µ ì„±ëŠ¥)",
            _ => "D (ì„±ëŠ¥ ê°œì„  í•„ìš”)",
        };
        
        info!("ğŸ† ì„±ëŠ¥ ë“±ê¸‰: {}", grade);
    }
    
    /// ë³‘ë ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ê³ ê¸‰)
    pub async fn run_parallel_benchmarks(&self) -> Result<HashMap<String, BenchmarkResult>> {
        info!("ğŸš€ ë³‘ë ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œì‘");
        
        // ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬ë“¤
        let handles = vec![
            tokio::spawn({
                let benchmark = self.clone();
                async move {
                    ("dashmap", benchmark.benchmark_dashmap_optimizer().await)
                }
            }),
            tokio::spawn({
                let benchmark = self.clone();
                async move {
                    ("simd", benchmark.benchmark_simd_optimizer().await)
                }
            }),
            tokio::spawn({
                let benchmark = self.clone();
                async move {
                    ("compression", benchmark.benchmark_message_compression().await)
                }
            }),
        ];
        
        let mut results = HashMap::new();
        
        for handle in handles {
            let (name, result) = handle.await?;
            match result {
                Ok(benchmark_result) => {
                    results.insert(name.to_string(), benchmark_result);
                }
                Err(e) => {
                    warn!("ë³‘ë ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨ {}: {}", name, e);
                }
            }
        }
        
        // ìˆœì°¨ ì‹¤í–‰ì´ í•„ìš”í•œ ë²¤ì¹˜ë§ˆí¬ë“¤
        if let Ok(async_io_result) = self.benchmark_async_io_optimizer().await {
            results.insert("async_io".to_string(), async_io_result);
        }
        
        if let Ok(monitor_result) = self.benchmark_performance_monitor().await {
            results.insert("monitoring".to_string(), monitor_result);
        }
        
        self.generate_summary_report(&results).await;
        Ok(results)
    }
}

impl Clone for PerformanceBenchmark {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            results: self.results.clone(),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_dashmap_benchmark() {
        let config = BenchmarkConfig {
            iterations: 100,
            ..Default::default()
        };
        
        let benchmark = PerformanceBenchmark::new(config);
        let result = benchmark.benchmark_dashmap_optimizer().await.unwrap();
        
        assert!(result.success_rate > 90.0);
        assert!(result.throughput_ops_per_sec > 0.0);
        assert!(result.performance_score() > 0.0);
    }
    
    #[tokio::test]
    async fn test_full_benchmark_suite() {
        let config = BenchmarkConfig {
            iterations: 50,
            ..Default::default()
        };
        
        let benchmark = PerformanceBenchmark::new(config);
        let results = benchmark.run_all_benchmarks().await.unwrap();
        
        assert!(!results.is_empty());
        assert!(results.contains_key("dashmap"));
        assert!(results.contains_key("compression"));
    }
}