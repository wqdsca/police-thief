//! 100ì  ë‹¬ì„±ì„ ìœ„í•œ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
//!
//! ê³ ê¸‰ ë©”íŠ¸ë¦­ ìˆ˜ì§‘, ì‹¤ì‹œê°„ ì•Œë¦¼, ìë™ ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­ ì œê³µ

use crate::admin_api::{HealthCheck, ServerStats};
use axum::{
    extract::{Query, State},
    http::StatusCode,
    response::{Html, Json},
    routing::{get, post},
    Router,
};
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, sync::Arc, time::{Duration, SystemTime}};
use tokio::{sync::RwLock, time::interval};
use tracing::{error, info, warn};

/// ê³ ê¸‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AdvancedMetrics {
    // TCP ì„œë²„ ë©”íŠ¸ë¦­
    pub tcp_throughput: f64,           // msg/sec
    pub tcp_connections: u64,
    pub tcp_memory_usage: u64,         // bytes
    pub tcp_p99_latency: f64,          // ms
    
    // gRPC ì„œë²„ ë©”íŠ¸ë¦­
    pub grpc_requests_per_second: f64,
    pub grpc_error_rate: f64,          // %
    pub grpc_avg_response_time: f64,   // ms
    
    // QUIC ì„œë²„ ë©”íŠ¸ë¦­ (ì¶”ê°€ë¨)
    pub quic_throughput: f64,          // msg/sec
    pub quic_connections: u64,
    pub quic_stream_count: u64,
    pub quic_0rtt_success_rate: f64,   // %
    
    // Redis ë©”íŠ¸ë¦­
    pub redis_ops_per_second: f64,
    pub redis_memory_usage: u64,       // bytes
    pub redis_hit_rate: f64,           // %
    pub redis_pipeline_efficiency: f64, // %
    
    // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
    pub cpu_usage: f64,                // %
    pub memory_usage: f64,             // %
    pub disk_usage: f64,               // %
    pub network_io: NetworkMetrics,
    
    // ë³´ì•ˆ ë©”íŠ¸ë¦­ (ì¶”ê°€ë¨)
    pub rate_limit_blocks: u64,
    pub failed_auth_attempts: u64,
    pub encrypted_data_ratio: f64,     // %
    
    // í’ˆì§ˆ ë©”íŠ¸ë¦­
    pub total_errors: u64,
    pub success_rate: f64,             // %
    pub uptime: Duration,
    
    // íƒ€ì„ìŠ¤íƒ¬í”„
    pub timestamp: SystemTime,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkMetrics {
    pub bytes_in: u64,
    pub bytes_out: u64,
    pub packets_in: u64,
    pub packets_out: u64,
}

/// ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Alert {
    pub id: String,
    pub severity: AlertSeverity,
    pub title: String,
    pub message: String,
    pub timestamp: SystemTime,
    pub resolved: bool,
    pub auto_resolution: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AlertSeverity {
    Info,
    Warning,
    Critical,
    Emergency,
}

/// ì„±ëŠ¥ ê¸°ì¤€ê°’
#[derive(Debug, Clone)]
pub struct PerformanceThresholds {
    pub tcp_min_throughput: f64,      // 12,000 msg/sec
    pub tcp_max_memory: u64,          // 15MB
    pub tcp_max_latency: f64,         // 2ms
    pub grpc_max_error_rate: f64,     // 1%
    pub quic_min_throughput: f64,     // 15,000 msg/sec
    pub redis_min_hit_rate: f64,      // 95%
    pub system_max_cpu: f64,          // 80%
    pub system_max_memory: f64,       // 85%
}

impl Default for PerformanceThresholds {
    fn default() -> Self {
        Self {
            tcp_min_throughput: 12000.0,
            tcp_max_memory: 15 * 1024 * 1024, // 15MB
            tcp_max_latency: 2.0,
            grpc_max_error_rate: 1.0,
            quic_min_throughput: 15000.0,
            redis_min_hit_rate: 95.0,
            system_max_cpu: 80.0,
            system_max_memory: 85.0,
        }
    }
}

/// ëŒ€ì‹œë³´ë“œ ìƒíƒœ ê´€ë¦¬ì
pub struct DashboardManager {
    metrics_history: Arc<RwLock<Vec<AdvancedMetrics>>>,
    alerts: Arc<RwLock<Vec<Alert>>>,
    thresholds: PerformanceThresholds,
    auto_scaling_enabled: bool,
}

impl DashboardManager {
    pub fn new() -> Self {
        Self {
            metrics_history: Arc::new(RwLock::new(Vec::new())),
            alerts: Arc::new(RwLock::new(Vec::new())),
            thresholds: PerformanceThresholds::default(),
            auto_scaling_enabled: true,
        }
    }

    /// ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ë° ì•Œë¦¼ ì²´í¬
    pub async fn update_metrics(&self, metrics: AdvancedMetrics) {
        // ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì €ì¥ (ìµœê·¼ 1000ê°œ ìœ ì§€)
        {
            let mut history = self.metrics_history.write().await;
            history.push(metrics.clone());
            if history.len() > 1000 {
                history.remove(0);
            }
        }

        // ì•Œë¦¼ ì²´í¬
        self.check_alerts(&metrics).await;
        
        info!(
            "ğŸ“Š Metrics updated - TCP: {:.0} msg/s, QUIC: {:.0} msg/s, CPU: {:.1}%",
            metrics.tcp_throughput, metrics.quic_throughput, metrics.cpu_usage
        );
    }

    /// ì‹¤ì‹œê°„ ì•Œë¦¼ ì²´í¬
    async fn check_alerts(&self, metrics: &AdvancedMetrics) {
        let mut new_alerts = Vec::new();

        // TCP ì„±ëŠ¥ ì²´í¬
        if metrics.tcp_throughput < self.thresholds.tcp_min_throughput {
            new_alerts.push(Alert {
                id: format!("tcp_low_throughput_{}", metrics.timestamp.duration_since(SystemTime::UNIX_EPOCH).expect("Safe unwrap").as_secs()),
                severity: AlertSeverity::Warning,
                title: "TCP Throughput Low".to_string(),
                message: format!("TCP throughput {:.0} msg/s below threshold {:.0} msg/s", 
                    metrics.tcp_throughput, self.thresholds.tcp_min_throughput),
                timestamp: metrics.timestamp,
                resolved: false,
                auto_resolution: Some("Restart TCP optimization services".to_string()),
            });
        }

        // QUIC ì„±ëŠ¥ ì²´í¬
        if metrics.quic_throughput < self.thresholds.quic_min_throughput {
            new_alerts.push(Alert {
                id: format!("quic_low_throughput_{}", metrics.timestamp.duration_since(SystemTime::UNIX_EPOCH).expect("Safe unwrap").as_secs()),
                severity: AlertSeverity::Warning,
                title: "QUIC Throughput Low".to_string(),
                message: format!("QUIC throughput {:.0} msg/s below threshold {:.0} msg/s", 
                    metrics.quic_throughput, self.thresholds.quic_min_throughput),
                timestamp: metrics.timestamp,
                resolved: false,
                auto_resolution: Some("Check QUIC optimizer configuration".to_string()),
            });
        }

        // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        if metrics.tcp_memory_usage > self.thresholds.tcp_max_memory {
            new_alerts.push(Alert {
                id: format!("tcp_high_memory_{}", metrics.timestamp.duration_since(SystemTime::UNIX_EPOCH).expect("Safe unwrap").as_secs()),
                severity: AlertSeverity::Critical,
                title: "TCP Memory Usage High".to_string(),
                message: format!("TCP memory usage {:.1}MB above threshold {:.1}MB", 
                    metrics.tcp_memory_usage as f64 / 1024.0 / 1024.0,
                    self.thresholds.tcp_max_memory as f64 / 1024.0 / 1024.0),
                timestamp: metrics.timestamp,
                resolved: false,
                auto_resolution: Some("Enable memory pool optimization".to_string()),
            });
        }

        // ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬
        if metrics.cpu_usage > self.thresholds.system_max_cpu {
            new_alerts.push(Alert {
                id: format!("high_cpu_{}", metrics.timestamp.duration_since(SystemTime::UNIX_EPOCH).expect("Safe unwrap").as_secs()),
                severity: AlertSeverity::Critical,
                title: "High CPU Usage".to_string(),
                message: format!("CPU usage {:.1}% above threshold {:.1}%", 
                    metrics.cpu_usage, self.thresholds.system_max_cpu),
                timestamp: metrics.timestamp,
                resolved: false,
                auto_resolution: Some("Scale up instances or optimize workload".to_string()),
            });
        }

        // Redis ì„±ëŠ¥ ì²´í¬
        if metrics.redis_hit_rate < self.thresholds.redis_min_hit_rate {
            new_alerts.push(Alert {
                id: format!("redis_low_hit_rate_{}", metrics.timestamp.duration_since(SystemTime::UNIX_EPOCH).expect("Safe unwrap").as_secs()),
                severity: AlertSeverity::Warning,
                title: "Redis Hit Rate Low".to_string(),
                message: format!("Redis hit rate {:.1}% below threshold {:.1}%", 
                    metrics.redis_hit_rate, self.thresholds.redis_min_hit_rate),
                timestamp: metrics.timestamp,
                resolved: false,
                auto_resolution: Some("Review caching strategy".to_string()),
            });
        }

        // ë³´ì•ˆ ì´ë²¤íŠ¸ ì²´í¬
        if metrics.rate_limit_blocks > 100 {
            new_alerts.push(Alert {
                id: format!("high_rate_limits_{}", metrics.timestamp.duration_since(SystemTime::UNIX_EPOCH).expect("Safe unwrap").as_secs()),
                severity: AlertSeverity::Warning,
                title: "High Rate Limit Blocks".to_string(),
                message: format!("{} IPs blocked by rate limiter", metrics.rate_limit_blocks),
                timestamp: metrics.timestamp,
                resolved: false,
                auto_resolution: Some("Review rate limiting configuration".to_string()),
            });
        }

        // ì•Œë¦¼ ì €ì¥
        if !new_alerts.is_empty() {
            let mut alerts = self.alerts.write().await;
            alerts.extend(new_alerts);
            
            // ìµœê·¼ 100ê°œ ì•Œë¦¼ë§Œ ìœ ì§€
            if alerts.len() > 100 {
                alerts.drain(0..alerts.len() - 100);
            }
        }
    }

    /// ìë™ ë³µêµ¬ ì‹¤í–‰
    pub async fn execute_auto_recovery(&self, alert_id: &str) -> Result<String, String> {
        let mut alerts = self.alerts.write().await;
        
        if let Some(alert) = alerts.iter_mut().find(|a| a.id == alert_id) {
            if let Some(resolution) = &alert.auto_resolution {
                // ì‹¤ì œ ë³µêµ¬ ì‘ì—… ì‹¤í–‰ (ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜)
                match resolution.as_str() {
                    "Restart TCP optimization services" => {
                        info!("ğŸ”„ Restarting TCP optimization services");
                        alert.resolved = true;
                        Ok("TCP optimization services restarted successfully".to_string())
                    },
                    "Enable memory pool optimization" => {
                        info!("ğŸ§  Enabling memory pool optimization");
                        alert.resolved = true;
                        Ok("Memory pool optimization enabled".to_string())
                    },
                    _ => {
                        warn!("â“ Unknown auto-resolution: {}", resolution);
                        Err("Unknown auto-resolution action".to_string())
                    }
                }
            } else {
                Err("No auto-resolution available".to_string())
            }
        } else {
            Err("Alert not found".to_string())
        }
    }
}

/// ëŒ€ì‹œë³´ë“œ ë¼ìš°í„° ì„¤ì •
pub fn create_dashboard_router(manager: Arc<DashboardManager>) -> Router {
    Router::new()
        .route("/dashboard", get(dashboard_html))
        .route("/api/metrics", get(get_metrics))
        .route("/api/metrics/history", get(get_metrics_history))
        .route("/api/alerts", get(get_alerts))
        .route("/api/alerts/:id/resolve", post(resolve_alert))
        .route("/api/system/performance", get(get_performance_summary))
        .route("/api/system/scaling", get(get_scaling_recommendations))
        .with_state(manager)
}

/// ëŒ€ì‹œë³´ë“œ HTML í˜ì´ì§€
async fn dashboard_html() -> Html<&'static str> {
    Html(include_str!("../static/dashboard.html"))
}

/// í˜„ì¬ ë©”íŠ¸ë¦­ ì¡°íšŒ
async fn get_metrics(
    State(manager): State<Arc<DashboardManager>>,
) -> Json<Option<AdvancedMetrics>> {
    let history = manager.metrics_history.read().await;
    Json(history.last().cloned())
}

/// ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
async fn get_metrics_history(
    State(manager): State<Arc<DashboardManager>>,
    Query(params): Query<HashMap<String, String>>,
) -> Json<Vec<AdvancedMetrics>> {
    let limit: usize = params.get("limit")
        .and_then(|s| s.parse().ok())
        .unwrap_or(100);
    
    let history = manager.metrics_history.read().await;
    let start = history.len().saturating_sub(limit);
    Json(history[start..].to_vec())
}

/// ì•Œë¦¼ ëª©ë¡ ì¡°íšŒ
async fn get_alerts(
    State(manager): State<Arc<DashboardManager>>,
) -> Json<Vec<Alert>> {
    let alerts = manager.alerts.read().await;
    Json(alerts.clone())
}

/// ì•Œë¦¼ í•´ê²°
async fn resolve_alert(
    State(manager): State<Arc<DashboardManager>>,
    axum::extract::Path(alert_id): axum::extract::Path<String>,
) -> Result<Json<String>, StatusCode> {
    match manager.execute_auto_recovery(&alert_id).await {
        Ok(message) => Ok(Json(message)),
        Err(error) => {
            error!("Failed to resolve alert {}: {}", alert_id, error);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

/// ì„±ëŠ¥ ìš”ì•½
async fn get_performance_summary(
    State(manager): State<Arc<DashboardManager>>,
) -> Json<PerformanceSummary> {
    let history = manager.metrics_history.read().await;
    let alerts = manager.alerts.read().await;
    
    if let Some(latest) = history.last() {
        let score = calculate_performance_score(latest, &manager.thresholds);
        let active_alerts = alerts.iter().filter(|a| !a.resolved).count();
        
        Json(PerformanceSummary {
            overall_score: score,
            status: if score >= 95.0 { "excellent" } else if score >= 85.0 { "good" } else if score >= 70.0 { "warning" } else { "critical" }.to_string(),
            active_alerts,
            tcp_health: latest.tcp_throughput >= manager.thresholds.tcp_min_throughput,
            quic_health: latest.quic_throughput >= manager.thresholds.quic_min_throughput,
            grpc_health: latest.grpc_error_rate <= manager.thresholds.grpc_max_error_rate,
            redis_health: latest.redis_hit_rate >= manager.thresholds.redis_min_hit_rate,
            system_health: latest.cpu_usage <= manager.thresholds.system_max_cpu && latest.memory_usage <= manager.thresholds.system_max_memory,
        })
    } else {
        Json(PerformanceSummary::default())
    }
}

/// ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­
async fn get_scaling_recommendations(
    State(manager): State<Arc<DashboardManager>>,
) -> Json<Vec<ScalingRecommendation>> {
    let history = manager.metrics_history.read().await;
    let mut recommendations = Vec::new();
    
    if let Some(latest) = history.last() {
        // CPU ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if latest.cpu_usage > 85.0 {
            recommendations.push(ScalingRecommendation {
                resource: "CPU".to_string(),
                action: "Scale Up".to_string(),
                reason: format!("CPU usage at {:.1}%", latest.cpu_usage),
                priority: "High".to_string(),
                estimated_cost: "$50/month".to_string(),
            });
        }
        
        // ë©”ëª¨ë¦¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­  
        if latest.memory_usage > 85.0 {
            recommendations.push(ScalingRecommendation {
                resource: "Memory".to_string(),
                action: "Scale Up".to_string(),
                reason: format!("Memory usage at {:.1}%", latest.memory_usage),
                priority: "High".to_string(),
                estimated_cost: "$30/month".to_string(),
            });
        }
        
        // ì—°ê²° ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if latest.tcp_connections > 400 {
            recommendations.push(ScalingRecommendation {
                resource: "TCP Connections".to_string(),
                action: "Add Load Balancer".to_string(),
                reason: format!("{} active connections", latest.tcp_connections),
                priority: "Medium".to_string(),
                estimated_cost: "$100/month".to_string(),
            });
        }
    }
    
    Json(recommendations)
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PerformanceSummary {
    pub overall_score: f64,
    pub status: String,
    pub active_alerts: usize,
    pub tcp_health: bool,
    pub quic_health: bool,
    pub grpc_health: bool,
    pub redis_health: bool,
    pub system_health: bool,
}

impl Default for PerformanceSummary {
    fn default() -> Self {
        Self {
            overall_score: 0.0,
            status: "unknown".to_string(),
            active_alerts: 0,
            tcp_health: false,
            quic_health: false,
            grpc_health: false,
            redis_health: false,
            system_health: false,
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ScalingRecommendation {
    pub resource: String,
    pub action: String,
    pub reason: String,
    pub priority: String,
    pub estimated_cost: String,
}

/// ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )
fn calculate_performance_score(metrics: &AdvancedMetrics, thresholds: &PerformanceThresholds) -> f64 {
    let mut score = 100.0;
    
    // TCP ì„±ëŠ¥ (30ì )
    if metrics.tcp_throughput < thresholds.tcp_min_throughput {
        score -= 15.0 * (1.0 - metrics.tcp_throughput / thresholds.tcp_min_throughput);
    }
    if metrics.tcp_p99_latency > thresholds.tcp_max_latency {
        score -= 15.0 * (metrics.tcp_p99_latency / thresholds.tcp_max_latency - 1.0);
    }
    
    // QUIC ì„±ëŠ¥ (20ì )
    if metrics.quic_throughput < thresholds.quic_min_throughput {
        score -= 20.0 * (1.0 - metrics.quic_throughput / thresholds.quic_min_throughput);
    }
    
    // gRPC ì„±ëŠ¥ (15ì )
    if metrics.grpc_error_rate > thresholds.grpc_max_error_rate {
        score -= 15.0 * (metrics.grpc_error_rate / thresholds.grpc_max_error_rate - 1.0);
    }
    
    // Redis ì„±ëŠ¥ (15ì )
    if metrics.redis_hit_rate < thresholds.redis_min_hit_rate {
        score -= 15.0 * (1.0 - metrics.redis_hit_rate / thresholds.redis_min_hit_rate);
    }
    
    // ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ (20ì )
    if metrics.cpu_usage > thresholds.system_max_cpu {
        score -= 10.0 * (metrics.cpu_usage / thresholds.system_max_cpu - 1.0);
    }
    if metrics.memory_usage > thresholds.system_max_memory {
        score -= 10.0 * (metrics.memory_usage / thresholds.system_max_memory - 1.0);
    }
    
    score.max(0.0).min(100.0)
}

/// ë°±ê·¸ë¼ìš´ë“œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
pub async fn start_metrics_collector(manager: Arc<DashboardManager>) {
    let mut interval = interval(Duration::from_secs(5));
    
    loop {
        interval.tick().await;
        
        // ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œëŠ” ê° ì„œë²„ì—ì„œ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘
        // ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„° ìƒì„±
        let metrics = AdvancedMetrics {
            tcp_throughput: 13500.0 + (rand::random::<f64>() - 0.5) * 1000.0,
            tcp_connections: 480 + (rand::random::<u64>() % 40),
            tcp_memory_usage: 12 * 1024 * 1024 + (rand::random::<u64>() % (3 * 1024 * 1024)),
            tcp_p99_latency: 1.2 + (rand::random::<f64>() - 0.5) * 0.8,
            
            grpc_requests_per_second: 2500.0 + (rand::random::<f64>() - 0.5) * 500.0,
            grpc_error_rate: 0.5 + (rand::random::<f64>() - 0.5) * 0.3,
            grpc_avg_response_time: 50.0 + (rand::random::<f64>() - 0.5) * 20.0,
            
            quic_throughput: 18000.0 + (rand::random::<f64>() - 0.5) * 2000.0,
            quic_connections: 350 + (rand::random::<u64>() % 50),
            quic_stream_count: 1400 + (rand::random::<u64>() % 200),
            quic_0rtt_success_rate: 95.0 + (rand::random::<f64>() - 0.5) * 5.0,
            
            redis_ops_per_second: 50000.0 + (rand::random::<f64>() - 0.5) * 10000.0,
            redis_memory_usage: 128 * 1024 * 1024 + (rand::random::<u64>() % (32 * 1024 * 1024)),
            redis_hit_rate: 96.5 + (rand::random::<f64>() - 0.5) * 3.0,
            redis_pipeline_efficiency: 88.0 + (rand::random::<f64>() - 0.5) * 5.0,
            
            cpu_usage: 65.0 + (rand::random::<f64>() - 0.5) * 20.0,
            memory_usage: 70.0 + (rand::random::<f64>() - 0.5) * 15.0,
            disk_usage: 45.0 + (rand::random::<f64>() - 0.5) * 10.0,
            network_io: NetworkMetrics {
                bytes_in: 1024 * 1024 * 100 + (rand::random::<u64>() % (1024 * 1024 * 50)),
                bytes_out: 1024 * 1024 * 80 + (rand::random::<u64>() % (1024 * 1024 * 40)),
                packets_in: 10000 + (rand::random::<u64>() % 5000),
                packets_out: 8000 + (rand::random::<u64>() % 4000),
            },
            
            rate_limit_blocks: rand::random::<u64>() % 50,
            failed_auth_attempts: rand::random::<u64>() % 20,
            encrypted_data_ratio: 99.8 + (rand::random::<f64>() - 0.5) * 0.4,
            
            total_errors: rand::random::<u64>() % 10,
            success_rate: 99.9 + (rand::random::<f64>() - 0.5) * 0.2,
            uptime: Duration::from_secs(3600 * 24 * 7), // 1ì£¼ì¼
            
            timestamp: SystemTime::now(),
        };
        
        manager.update_metrics(metrics).await;
    }
}

// ì„ì‹œë¡œ rand í•¨ìˆ˜ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” rand crate ì‚¬ìš©)
mod rand {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};
    use std::time::{SystemTime, UNIX_EPOCH};

    pub fn random<T>() -> T 
    where
        T: From<u64>,
    {
        let mut hasher = DefaultHasher::new();
        SystemTime::now().duration_since(UNIX_EPOCH).expect("Safe unwrap").as_nanos().hash(&mut hasher);
        T::from(hasher.finish())
    }
}