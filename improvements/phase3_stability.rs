// üõ°Ô∏è Phase 3: ÏïàÏ†ïÏÑ± 100Ï†ê Îã¨ÏÑ±
// NetflixÏùò Hystrix, Resilience4j Ìå®ÌÑ¥ Ï†ÅÏö©

use std::sync::Arc;
use tokio::sync::{RwLock, Semaphore};
use std::time::{Duration, Instant};
use std::collections::VecDeque;

// ‚úÖ 1. Circuit Breaker Ìå®ÌÑ¥
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum CircuitState {
    Closed,
    Open,
    HalfOpen,
}

pub struct CircuitBreaker {
    state: Arc<RwLock<CircuitState>>,
    failure_count: Arc<RwLock<u32>>,
    last_failure_time: Arc<RwLock<Option<Instant>>>,
    config: CircuitBreakerConfig,
}

pub struct CircuitBreakerConfig {
    pub failure_threshold: u32,
    pub success_threshold: u32,
    pub timeout: Duration,
    pub half_open_max_calls: u32,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        Self {
            state: Arc::new(RwLock::new(CircuitState::Closed)),
            failure_count: Arc::new(RwLock::new(0)),
            last_failure_time: Arc::new(RwLock::new(None)),
            config,
        }
    }
    
    pub async fn call<F, T, E>(&self, f: F) -> Result<T, E>
    where
        F: FnOnce() -> Result<T, E>,
        E: std::error::Error,
    {
        let state = *self.state.read().await;
        
        match state {
            CircuitState::Open => {
                // Check if we should transition to half-open
                if let Some(last_failure) = *self.last_failure_time.read().await {
                    if last_failure.elapsed() > self.config.timeout {
                        *self.state.write().await = CircuitState::HalfOpen;
                        *self.failure_count.write().await = 0;
                    } else {
                        return Err(Self::circuit_open_error());
                    }
                }
            }
            CircuitState::HalfOpen => {
                // Limited calls in half-open state
                let failures = *self.failure_count.read().await;
                if failures >= self.config.half_open_max_calls {
                    *self.state.write().await = CircuitState::Open;
                    return Err(Self::circuit_open_error());
                }
            }
            CircuitState::Closed => {}
        }
        
        match f() {
            Ok(result) => {
                self.on_success().await;
                Ok(result)
            }
            Err(e) => {
                self.on_failure().await;
                Err(e)
            }
        }
    }
    
    async fn on_success(&self) {
        let mut state = self.state.write().await;
        let mut failures = self.failure_count.write().await;
        
        if *state == CircuitState::HalfOpen {
            *failures = 0;
            *state = CircuitState::Closed;
        }
    }
    
    async fn on_failure(&self) {
        let mut failures = self.failure_count.write().await;
        *failures += 1;
        
        if *failures >= self.config.failure_threshold {
            *self.state.write().await = CircuitState::Open;
            *self.last_failure_time.write().await = Some(Instant::now());
        }
    }
    
    fn circuit_open_error<E: std::error::Error>() -> E {
        // Create appropriate error
        unimplemented!()
    }
}

// ‚úÖ 2. Bulkhead Ìå®ÌÑ¥ (Î¶¨ÏÜåÏä§ Í≤©Î¶¨)
pub struct Bulkhead {
    semaphore: Arc<Semaphore>,
    name: String,
}

impl Bulkhead {
    pub fn new(name: String, max_concurrent: usize) -> Self {
        Self {
            semaphore: Arc::new(Semaphore::new(max_concurrent)),
            name,
        }
    }
    
    pub async fn execute<F, T>(&self, f: F) -> Result<T, BulkheadError>
    where
        F: std::future::Future<Output = T>,
    {
        let permit = self.semaphore
            .try_acquire()
            .map_err(|_| BulkheadError::NoCapacity(self.name.clone()))?;
        
        let result = f.await;
        drop(permit);
        Ok(result)
    }
}

#[derive(Debug, thiserror::Error)]
pub enum BulkheadError {
    #[error("Bulkhead {0} has no capacity")]
    NoCapacity(String),
}

// ‚úÖ 3. Retry with Exponential Backoff
pub struct RetryStrategy {
    max_attempts: u32,
    base_delay: Duration,
    max_delay: Duration,
    jitter: bool,
}

impl RetryStrategy {
    pub async fn execute<F, T, E, Fut>(&self, mut f: F) -> Result<T, E>
    where
        F: FnMut() -> Fut,
        Fut: std::future::Future<Output = Result<T, E>>,
        E: std::error::Error,
    {
        let mut attempt = 0;
        let mut delay = self.base_delay;
        
        loop {
            attempt += 1;
            
            match f().await {
                Ok(result) => return Ok(result),
                Err(e) if attempt >= self.max_attempts => return Err(e),
                Err(_) => {
                    if self.jitter {
                        use rand::Rng;
                        let jitter = rand::thread_rng().gen_range(0..=delay.as_millis() as u64 / 4);
                        delay += Duration::from_millis(jitter);
                    }
                    
                    tokio::time::sleep(delay).await;
                    delay = std::cmp::min(delay * 2, self.max_delay);
                }
            }
        }
    }
}

// ‚úÖ 4. Health Check System
use async_trait::async_trait;

#[async_trait]
pub trait HealthCheck: Send + Sync {
    async fn check(&self) -> HealthStatus;
    fn name(&self) -> &str;
}

#[derive(Debug, Clone)]
pub enum HealthStatus {
    Healthy,
    Degraded(String),
    Unhealthy(String),
}

pub struct HealthMonitor {
    checks: Vec<Box<dyn HealthCheck>>,
}

impl HealthMonitor {
    pub async fn check_all(&self) -> SystemHealth {
        let mut results = Vec::new();
        
        for check in &self.checks {
            let status = check.check().await;
            results.push((check.name().to_string(), status));
        }
        
        SystemHealth { results }
    }
}

pub struct SystemHealth {
    pub results: Vec<(String, HealthStatus)>,
}

impl SystemHealth {
    pub fn is_healthy(&self) -> bool {
        self.results.iter().all(|(_, status)| {
            matches!(status, HealthStatus::Healthy)
        })
    }
}

// ‚úÖ 5. Graceful Shutdown
use tokio::signal;
use tokio::sync::broadcast;

pub struct GracefulShutdown {
    shutdown_tx: broadcast::Sender<()>,
    tasks: Arc<RwLock<Vec<tokio::task::JoinHandle<()>>>>,
}

impl GracefulShutdown {
    pub fn new() -> Self {
        let (shutdown_tx, _) = broadcast::channel(1);
        Self {
            shutdown_tx,
            tasks: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn register_task(&self, handle: tokio::task::JoinHandle<()>) {
        self.tasks.write().await.push(handle);
    }
    
    pub async fn shutdown(self) {
        info!("Initiating graceful shutdown...");
        
        // Send shutdown signal
        let _ = self.shutdown_tx.send(());
        
        // Wait for all tasks with timeout
        let tasks = self.tasks.read().await;
        let shutdown_future = futures::future::join_all(tasks.iter().map(|_| async {
            // Wait for task completion
        }));
        
        if tokio::time::timeout(Duration::from_secs(30), shutdown_future).await.is_err() {
            warn!("Some tasks did not complete within shutdown timeout");
        }
        
        info!("Graceful shutdown complete");
    }
    
    pub fn subscribe(&self) -> broadcast::Receiver<()> {
        self.shutdown_tx.subscribe()
    }
}

// ‚úÖ 6. Error Recovery System
pub struct ErrorRecovery {
    strategies: Vec<Box<dyn RecoveryStrategy>>,
}

#[async_trait]
pub trait RecoveryStrategy: Send + Sync {
    async fn can_recover(&self, error: &dyn std::error::Error) -> bool;
    async fn recover(&self, error: &dyn std::error::Error) -> Result<(), Box<dyn std::error::Error>>;
}

// ‚úÖ 7. Timeout Management
pub struct TimeoutManager {
    default_timeout: Duration,
    operation_timeouts: std::collections::HashMap<String, Duration>,
}

impl TimeoutManager {
    pub async fn with_timeout<F, T>(&self, operation: &str, future: F) -> Result<T, TimeoutError>
    where
        F: std::future::Future<Output = T>,
    {
        let timeout = self.operation_timeouts
            .get(operation)
            .copied()
            .unwrap_or(self.default_timeout);
        
        tokio::time::timeout(timeout, future)
            .await
            .map_err(|_| TimeoutError {
                operation: operation.to_string(),
                timeout,
            })
    }
}

#[derive(Debug, thiserror::Error)]
#[error("Operation {operation} timed out after {timeout:?}")]
pub struct TimeoutError {
    pub operation: String,
    pub timeout: Duration,
}

// ‚úÖ 8. Monitoring & Metrics
use prometheus::{IntCounter, Histogram, register_int_counter, register_histogram};

pub struct MetricsCollector {
    pub requests_total: IntCounter,
    pub errors_total: IntCounter,
    pub request_duration: Histogram,
}

impl MetricsCollector {
    pub fn new() -> Result<Self, prometheus::Error> {
        Ok(Self {
            requests_total: register_int_counter!("requests_total", "Total requests")?,
            errors_total: register_int_counter!("errors_total", "Total errors")?,
            request_duration: register_histogram!("request_duration_seconds", "Request duration")?,
        })
    }
    
    pub fn record_request(&self, duration: Duration, success: bool) {
        self.requests_total.inc();
        if !success {
            self.errors_total.inc();
        }
        self.request_duration.observe(duration.as_secs_f64());
    }
}

// ‚úÖ 9. Distributed Tracing
use opentelemetry::{trace::{Tracer, SpanKind}, global};
use tracing_opentelemetry::OpenTelemetryLayer;

pub fn init_tracing() -> Result<(), Box<dyn std::error::Error>> {
    global::set_text_map_propagator(opentelemetry_jaeger::Propagator::new());
    
    let tracer = opentelemetry_jaeger::new_pipeline()
        .with_service_name("game-server")
        .install_batch(opentelemetry::runtime::Tokio)?;
    
    let telemetry = OpenTelemetryLayer::new(tracer);
    
    tracing_subscriber::registry()
        .with(telemetry)
        .with(tracing_subscriber::fmt::layer())
        .init();
    
    Ok(())
}

// ‚úÖ 10. Chaos Engineering Support
#[cfg(feature = "chaos")]
pub mod chaos {
    use rand::Rng;
    
    pub struct ChaosMonkey {
        failure_rate: f32,
        latency_ms: Option<u64>,
    }
    
    impl ChaosMonkey {
        pub async fn maybe_fail(&self) -> Result<(), ChaosError> {
            let mut rng = rand::thread_rng();
            
            if rng.gen::<f32>() < self.failure_rate {
                return Err(ChaosError::RandomFailure);
            }
            
            if let Some(latency) = self.latency_ms {
                let actual_latency = rng.gen_range(0..=latency);
                tokio::time::sleep(Duration::from_millis(actual_latency)).await;
            }
            
            Ok(())
        }
    }
    
    #[derive(Debug, thiserror::Error)]
    pub enum ChaosError {
        #[error("Random failure injected by chaos monkey")]
        RandomFailure,
    }
}